# A Novel Method for Decoding Any High-Order Hidden Markov Model

## Metadata

- **Authors:** Fei Ye¹², Yifei Wang³
- **Affiliations:**
  - ¹ Computational Experiment Center for Social Science, Nanjing University, Nanjing 210093, China
  - ² School of Mathematics and Computer Science, Tongling University, Tongling, Anhui 244061, China
  - ³ Department of Mathematics, Shanghai University, Shanghai 200444, China
- **Correspondence:** Fei Ye; postyf@163.com
- **Journal:** Discrete Dynamics in Nature and Society
- **Volume:** 2014
- **Article ID:** 231704
- **DOI:** http://dx.doi.org/10.1155/2014/231704
- **Year:** 2014
- **Dates:** Received 9 August 2014; Revised 19 October 2014; Accepted 11 November 2014; Published 23 November 2014
- **Academic Editor:** Weiming Xiang
- **Copyright:** © 2014 F. Ye and Y. Wang. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.

## Abstract

This paper proposes a novel method for decoding any high-order hidden Markov model. First, the high-order hidden Markov model is transformed into an equivalent first-order hidden Markov model by Hadar's transformation. Next, the optimal state sequence of the equivalent first-order hidden Markov model is recognized by the existing Viterbi algorithm of the first-order hidden Markov model. Finally, the optimal state sequence of the high-order hidden Markov model is inferred from the optimal state sequence of the equivalent first-order hidden Markov model. This method provides a unified algorithm framework for decoding hidden Markov models including the first-order hidden Markov model and any high-order hidden Markov model.

## 1. Introduction

Hidden Markov models are powerful tools for modeling and analyzing sequential data. For several decades, hidden Markov models have been used in many fields including handwriting recognition [1-3], speech recognition [4, 5], computational biology [6, 7], and longitudinal data analysis [8, 9]. Past and current developments on hidden Markov models are well documented in [10, 11]. A hidden Markov model comprises an underlying Markov chain and an observed process, where the observed process is a probabilistic function of the underlying Markov chain [12]. Given a hidden Markov model, an efficient procedure for finding the optimal state sequence is of great interest in the real-world applications. In the traditional first-order hidden Markov model, the Viterbi algorithm is utilized to recognize the optimal state sequence [13]. Like the Kalman filter, the Viterbi algorithm tracks the optimal state sequence with a recursive method.

In recent years, the theory and applications of high-order hidden Markov models have been substantially advanced, and high-order hidden Markov models are known to be more powerful than the first-order hidden Markov model. There are two basic approaches to study the algorithms of high-order hidden Markov models. The first one is called the extended approach, which is to extend directly the existing algorithms of the first-order hidden Markov model to high-order hidden Markov models [14-16]. The second one is called the model reduction method, which is to transform a high-order hidden Markov model to an equivalent first-order hidden Markov model by some means and then to establish the algorithms of the high-order hidden Markov model by using standard techniques applicable to the first-order hidden Markov model [17-20].

In this paper, we propose a novel method for decoding any high-order hidden Markov model. First, the high-order hidden Markov model is transformed into an equivalent first-order hidden Markov model by Hadar's transformation. Next, the optimal state sequence of the equivalent first-order hidden Markov model is recognized by the existing Viterbi algorithm of the first-order hidden Markov model. Finally, the optimal state sequence of the high-order hidden Markov model is inferred from the optimal state sequence of the equivalent first-order hidden Markov model.

## Methods

### 2. High-Order Hidden Markov Model and Hadar's Transformation

Initially suppose two processes {q} and {o,} are defined on some probability space (Ω, F, P), where t is an integer index. q̄t takes values in a finite set S = {s₀, s₁, . . ., s\_{N−1}}, and oₜ takes values in a finite set V = {v₁, v₂, ..., v_M}. Without loss of generality, the elements of S can be denoted by {0, 1, . . ., N − 1}. A high-order hidden Markov model is defined as follows.

**Definition 1** (see [18, 20]). A high-order hidden Markov model is a doubly stochastic process with an underlying state process that is not directly observable but can be observed only through another stochastic process that is called the observation process. The observation process is governed by the hidden state process and produces the observation sequence. The state process and observation process, respectively, satisfy the following conditions.

(a) The hidden state process {q̄t} is a homogeneous Markov chain of order n, that is, a stochastic process that satisfies
P (q̄t | {q̄l}₁<t) = P (q̄t | {q̄l}t−n<l<t). (1)

(b) The observation process {oₜ} is governed by the hidden state process according to a set of probability distributions that satisfy
P (oₜ | {o₁}₁<t, {q̄l}₁≤t) = P (oₜ | {q̄l}t−(m−1)≤l≤t). (2)

To model the high-order hidden Markov model, the following parameters are needed.

(1) State transition probability distribution:
ā*{i_n...i_1 i_0} = P (q̄t = i₀ | q̄*{t−1} = i₁, . . ., q̄\_{t−n} = i_n) . (3)

(2) Symbol emission probability distribution:
b̄*{i*{m−1}...i*0}(l) = P (oₜ = v_l | q̄t = i₀, . . ., q̄*{t−(m−1)} = i\_{m−1}). (4)

(3) Initial state probability distribution:
π̄*{i_r...i_1} = P (q̄₁ = i₁, ..., q̄*{2−r} = i_r), (5)

where r = max{n, m}, i₀, i₁, ..., i*r ∈ S, and v_l ∈ V. We call Ā = {ā*{i*n...i_1 i_0}}, B̄ = {b̄*{i*{m-1}...i_0}(l)}, and π̄ = {π̄*{i_r...i_1}}. For convenience, we use the compact notation Λ̄ = (π̄, Ā, B̄) to indicate the complete parameters of the high-order hidden Markov model.

**Definition 2** (see [18]). Let
f : Ŝ = {0, 1, ..., N − 1}^r → S = {0, 1, ..., N^r − 1} (6)
be the mapping of any base N number to its decimal value; that is, if [i₀, i₁, . . ., i_{r−1}] ∈ Ŝ, then
f ([i₀, i₁, ..., i_{r−1}]) = ∑\_{l=0}^{r−1} i_l N^{r−1−l}. (7)

**Definition 3** (see [17]). Any two models λ₁ and λ₂ are defined as equivalent if
P (O | λ₁) = P (O | λ₂) (8)
for any arbitrary observation sequence O. In other words, two models are only considered equivalent if they yield the same likelihood, regardless of the specific observation sequence.

Based on Definition 2, we set
qₜ = f ([q̄t, q̄_{t−1}, . . ., q̄_{t−(r−1)}]) = ∑*{l=0}^{r−1} q̄*{t−l} N^{r−1−l}. (9)

Since q̄t, q̄*{t−1}, ..., and q̄*{t−(r−1)} take values in the set S = {0, 1, ..., N − 1}, qₜ takes values in the set S = {0, 1, ..., N^r − 1}. Moreover, it is easy to see that the inverse transformation can be implemented as follows:
q̄t = [ qₜ / N^{r−1} ],
q̄*{t−1} = [ (qₜ − q̄t N^{r−1}) / N^{r−2} ],
...
q̄*{t−(r−1)} = qₜ − q̄t N^{r−1} − ··· − q̄\_{t−(r−2)}N. (10)

**Remark 4.** The function f is a one to one correspondence between the set Ŝ = {0, 1, ..., N − 1}^r and the set S = {0, 1, ..., N^r − 1}.

**Proposition 5** (see [18]). Let a*{ij} = P(q*{t+1} = j | qₜ = i) for any i, j ∈ S. If [i/N] ≠ j − [j/N^{r−1}]N^{r−1}, then a*{ij} = 0; that is, a transition from qₜ = i to q*{t+1} = j is impossible.

**Lemma 6.** Let qₜ = ∑*{l=0}^{r−1} q̄*{t−l} N^{r−1−l}; then the process {qₜ} forms the first-order homogeneous Markov chain.

_Proof._ Without loss of generality, we may assume that qₜ = i and q*{t+1} = j, where i, j ∈ S.
First, we consider the case that [i/N] ≠ j − [j/N^{r−1}]N^{r−1}. By Proposition 5, it is easy to see that
P(q*{t+1} = j | qₜ = i, q*{t−1} = …) = P (q*{t+1} = j | qₜ = i) = 0. (11)
Next, we consider the case that [i/N] = j − [j/N^{r−1}]N^{r−1}. Since qₜ = i and q*{t+1} = j, it follows from (10) that
q̄*{t+1} = i₀,
q̄t = i₁,
...
q̄*{t+1−(r−1)} = i*{r−1},
q̄*{t−(r−1)} = i_r, (12)
where i₀, i₁, . . ., i*{r−1}, i*r ∈ S̄. Moreover, we have
P (q*{t+1} = j | qₜ = i, q*{t−1} = …)
= P (q̄*{t+1} = i₀, q̄t = i₁, ..., q̄*{t+1−(r−1)} = i*{r−1} | q̄t = i₁, ..., q̄*{t−(r−2)} = i*{r−1}, q̄*{t−(r−1)} = i_r, q̄*{t−r} = ···)
= P (q̄*{t+1} = i₀ | q̄t = i₁, ..., q̄*{t−(n−2)} = i*{n−1}, q̄*{t−(n−1)} = i*n)
= P (q̄*{t+1} = i₀ | q̄t = i₁, ..., q̄*{t−(r−2)} = i*{r−1}, q̄*{t−(r−1)} = i_r)
= P (q̄*{t+1} = i₀, q̄t = i₁, ..., q̄*{t−(r−2)} = i*{r−1} | q̄t = i₁, ..., q̄*{t−(r−2)} = i*{r−1}, q̄*{t−(r−1)} = i_r)
= P (q*{t+1} = j | qₜ = i). (13)
Through the above analysis, we derive that
P (q*{t+1} | {q_l}*{1≤l≤t+1}) = P (q*{t+1} | qₜ). (14)
Analogously, it is easy to see that
P (q*{t+1} = j | qₜ = i) = P (qₜ = j | q\_{t−1} = i). (15)
Therefore, the process {qₜ} forms the first-order homogeneous Markov chain. □

**Lemma 7.** The two processes {qₜ} and {oₜ} form the first-order hidden Markov model.

_Proof._ Without loss of generality, we may assume that oₜ = v*l and qₜ = i, where v_l ∈ V and i ∈ S. Since qₜ = i, it follows from (10) that
q̄t = i₁,
q̄*{t−1} = i₂,
...
q̄*{t−(r−1)} = i_r, (16)
where i₁, i₂, ..., i_r ∈ S̄. Moreover, we have
P (oₜ = v_l | {o_l}*{1<t}, {q*l}*{1<t}, qₜ = i)
= P (oₜ = v*l | {o_l}*{1<t}, q̄t = i₁, q̄*{t−1} = i₂, ..., q̄*{t−(r−1)} = i*r, q̄*{t−(r−2)} = …)
= P (oₜ = v*l | q̄t = i₁, q̄*{t−1} = i₂, ..., q̄*{t−(m−1)} = i_m)
= P (oₜ = v_l | q̄t = i₁, q̄*{t−1} = i₂, ..., q̄*{t−(r−1)} = i_r)
= P (oₜ = v_l | qₜ = i). (17)
Analogously, it is easy to see that
P (oₜ = v_l | qₜ = i) = P (o*{t−1} = v*l | q*{t−1} = i). (18)
Combining these with Lemma 6, we prove that the two processes {qₜ} and {oₜ} form the first-order hidden Markov model. □

**Remark 8.** Hadar and Messer [18] had also mentioned the fact that the two processes {qₜ} and {oₜ} form the first-order hidden Markov model, but they did not discuss and prove it in detail.

To model the first-order hidden Markov model {qₜ, oₜ}, the following parameters are needed.
(1) State transition probability distribution:
A*{ij} = P (q*{t+1} = j | qₜ = i). (19)
(2) Symbol emission probability distribution:
bᵢ(l) = P (oₜ = v*l | qₜ = i). (20)
(3) Initial state probability distribution:
πᵢ = P (q₁ = i), (21)
where i, j ∈ S and v_l ∈ V. We call A = {a*{ij}}, B = {bᵢ(l)}, and π = {πᵢ}. For convenience, we use the compact notation λ = (π, A, B) to indicate the complete parameters of the first-order hidden Markov model.

**Proposition 9** (see [18]). Let i = f([i₁, ..., i_r]) and j = f([i₀, ..., i_{r−1}]) for any i₀, i₁, ..., i*r ∈ S̄; then
π̄*{i*r...i_1} = πᵢ, ā*{i*r...i_1 i_0} = a*{ij}, b̄\_{i_r...i_1}(l) = bᵢ(l). (22)

**Lemma 10.** Let O = o₁ … o_T be any arbitrary observation sequence; then
P (O | Λ̄) = P (O | λ). (23)
That is, the high-order hidden Markov model {q̄t, oₜ} is equivalent to the first-order hidden Markov model {qₜ, oₜ}.

_Proof._ For any i*{2−r}, ..., i₁, ..., i*{T−(r−1)}, ..., i*T ∈ S̄, let
j₁ = f ([i₁, ..., i*{2−r}]), ..., j*T = f ([i_T, ..., i*{T−(r−1)}]). (24)
By Proposition 9, we have
P(O | Λ̄)
= ∑*{i*{2−r}=0}^{N−1} … ∑*{i_T=0}^{N−1} P (o₁ … o_T, q̄*{2−r} = i*{2−r}, . . ., q̄_T = i_T | Λ̄)
= ∑*{i*{2−r}=0}^{N−1} … ∑*{i*T=0}^{N−1} π̄*{i*r...i_1} b̄*{i*m...i_1}(o₁) ā*{i*n...i_1 i_0} × b̄*{i*{m+1}...i_2}(o₂) … ā*{i*{T−n+1}...i*{T−1} i*T} b̄*{i*{T−m+1}...i_T}(o_T)
= ∑*{j₁=0}^{N^r−1} … ∑*{j_T=0}^{N^r−1} π*{j₁} b*{j₁}(o₁) a*{j₁j₂} b*{j₂}(o₂) … a*{j*{T−1}j_T} b*{j*T}(o_T)
= ∑*{j₁=0}^{N^r−1} … ∑\_{j_T=0}^{N^r−1} P (o₁ … o_T, q₁ = j₁, . . ., q_T = j_T | λ)
= P (O | λ). (25) □

**Remark 11.** Hadar and Messer [18] had also mentioned the fact that the high-order hidden Markov model {q̄t, oₜ} is equivalent to the first-order hidden Markov model {qₜ, oₜ}, but they did not discuss and prove it in detail.

### 3. Methodology

**Theorem 12.** Let O = o₁ … o*T be any given observation sequence, and assume that qₜ = f([q̄t, q̄*{t−1}, . . ., q̄*{t−(r−1)}]) = ∑*{l=0}^{r−1} q̄*{t−l} N^{r−1−l}; then
P (q̄*{2−r} … q̄_T | O, Λ̄) = P (q₁ … q_T | O, λ). (26)

_Proof._ Without loss of generality, let q̄*{2−r} = i*{2−r}, . . ., q̄*T = i_T, where i*{2−r}, ..., i*T ∈ S̄. According to Proposition 9, we have
P (o₁ … o_T, q̄*{2−r} = i*{2−r}, ..., q̄_T = i_T | Λ̄)
= π̄*{i*r...i_1} b̄*{i*m...i_1}(o₁) ā*{i*n...i_1 i_0} × b̄*{i*{m+1}...i_2}(o₂) … ā*{i*{T−n+1}...i*{T−1} i*T} b̄*{i*{T−m+1}...i_T}(o_T)
= π*{j₁} b*{j₁}(o₁) a*{j₁j₂} b*{j₂}(o₂) … a*{j*{T−1}j_T} b*{j*T}(o_T)
= P (o₁ … o_T, q₁ = j₁, . . ., q_T = j_T | λ), (27)
where jₜ = f([i_t, ..., i*{t−(r−1)}]) for 1 ≤ t ≤ T.
On the other hand, it is easy to see that
P (q̄*{2−r} = i*{2−r}, . . ., q̄*T = i_T | O, Λ̄)
= P (O, q̄*{2−r} = i*{2−r}, . . ., q̄_T = i_T | Λ̄) / P(O | Λ̄)
= P (O, q₁ = j₁, . . ., q_T = j_T | λ) / P(O | λ)
= P (q₁ = j₁, . . ., q_T = j_T | O, λ) (28)
Hence, by Lemma 10, we have
P (q̄*{2−r} … q̄_T | O, Λ̄) = P (q₁ … q_T | O, λ). (29) □

**Theorem 13.** Let O = o₁ … o*T be any given observation sequence, and assume that the state sequence Q̄* = q̄**{2−r} … q̄*\_T satisfies
Q̄* = argmax*Q̄ P (Q̄, O | Λ̄); (30)
that is, the state sequence Q̄* = q̄**{2−r} … q̄*\_T is some optimal state sequence of the high-order hidden Markov model {q̄t, oₜ}. Let q*ₜ = f[q̄*t … q̄*_{t−(r−1)}] (1 ≤ t ≤ T); then the state sequence Q* = q*₁ … q*\_T satisfies
P (Q*, O | λ) = max_Q P (Q, O | λ). (31)
That is, the state sequence Q* = q*₁ … q\*\_T is some optimal state sequence of the first hidden Markov model {qₜ, oₜ}.

_Proof._ By Theorem 12, it is easy to see that
P (Q̄*, O | Λ̄) = P (Q*, O | λ), (32)
max_Q̄ P (Q̄, O | Λ̄) = max_Q P (Q, O | λ).
Meanwhile, we have the equation
P (Q̄*, O | Λ̄) = max_Q̄ P (Q̄, O | Λ̄). (33)
Hence, we drive that
P(Q*, O | λ) = max_Q P (Q, O | λ). (34) □

According to Theorem 13, we can know that some optimal state sequence of the high-order hidden Markov model {q̄t, oₜ} is mapped to some optimal state sequence of the first-order hidden Markov model {qₜ, oₜ}. Similarly, we can draw the following conclusion.

**Theorem 14.** Let O = o₁ … o*T be any given observation sequence, and assume that the state sequence Q* = q*₁ … q*\_T satisfies
Q* = argmax_Q P (Q, O | λ); (35)
that is, the state sequence Q* = q*₁ … q*\_T is some optimal state sequence of the first-order hidden Markov model {qₜ, oₜ}. For 1 ≤ t ≤ T, let
q̄*t = [ q*ₜ / N^{r−1} ],
q̄\**{t−1} = [ (q*ₜ − q̄*t N^{r−1}) / N^{r−2} ],
...
q̄*\_{t−(r−1)} = q*ₜ − q̄*t N^{r−1} − ··· − q̄*\_{t−(r−2)}N; (36)
then the state sequence Q̄* = q̄*₁ … q̄*\_T satisfies
P(Q̄*, O | Λ̄) = max_Q̄ P (Q̄, O | Λ̄). (37)
That is, the state sequence Q̄* = q̄\*₁ … q̄\*\_T is some optimal state sequence of the high-order hidden Markov model {q̄t, oₜ}.

**Remark 15.** Combining Theorem 13 with Theorem 14, it is known that there exists a one to one correspondence between the optimal state sequence of the high-order hidden Markov model {q̄t, oₜ} and the optimal state sequence of the first-order hidden Markov model {qₜ, oₜ}.

To decode any high-order hidden Markov model {q̄t, oₜ}, we transform it into an equivalent first-order hidden Markov model {qₜ, oₜ} by Hadar's transformation. Then, do as follows.

_Step 1._ Determine some optimal state sequence Q* = q*₁ q*₂ … q*\_T of the first-order hidden Markov model {qₜ, oₜ} by using the Viterbi algorithm. Without loss of generality, let
q*₁ = j₁,
q*₂ = j₂,
...
q\*\_T = j_T, (38)
where j₁, j₂, ..., j_T ∈ S.

_Step 2._ For q*ₜ = j_T, using the transformations
q̄*_T = [ j_T / N^{r−1} ],
q̄\*_{T−1} = [ (j_T − q̄*_T N^{r−1}) / N^{r−2} ],
...
q̄*\_{T−(r−1)} = j_T − q̄*_T N^{r−1} − ··· − q̄\*_{T−(r−2)}N, (39)
we have
q̄*\_T = [ j_T / N^{r−1} ],
q̄*_{T−1} = [ (j_T − q̄*_T N^{r−1}) / N^{r−2} ],
...
q̄\*_{T−(r−2)} = [ (j_T - ∑_{l=0}^{r−3} q̄*_{T−l} N^{r−1−l}) / N ],
q̄\*_{T−(r−1)} = j_T − ∑_{l=0}^{r−2} q̄\*\_{T−l} N^{r−1−l}. (40)

_Step 3._ For q*₁ = j₁, q*₂ = j₂, . . ., q**{T−1} = j*{T−1}, using the transformation
q*ₜ = ∑\*{l=0}^{r−1} q̄\*\*{t−l} N^{r−1−l}, (41)
we have
q̄**{T−r} = j*{T−1} − ∑\_{l=1}^{r−1} q̄*_{T−l} N^{r−l},
q̄\*_{T−(r+1)} = j*{T−2} − ∑*{l=1}^{r−1} q̄*\_{T−1−l} N^{r−l},
...
q̄*_{2−r} = j₁ − ∑_{l=1}^{r−1} q̄\*\_{2−l} N^{r−l}. (42)

## Results

(The primary result is the method itself and the derived optimal state sequence described below, culminating from the methodology section.)

Combining Step 2 with Step 3, we obtain the state sequence
Q̄* = q̄*_{2−r} q̄\*_{3−r} … q̄*\_{T−1} q̄*_T (43)
According to Theorem 14, the above state sequence Q̄* = q̄*_{2−r} q̄*\_{3−r} … q̄*\_{T−1} q̄\*\_T is some optimal state sequence of the high-order hidden Markov model {q̄t, oₜ}.

## 4. Conclusions

In this paper, a novel method for decoding any high-order hidden Markov model is given. Based on this method, the optimal state sequence of any high-order hidden Markov model can be inferred by the existing Viterbi algorithm of the first-order hidden Markov model. This method has universal character for decoding hidden Markov models and provides a unified algorithm framework for decoding hidden Markov models including the first-order hidden Markov model and any high-order hidden Markov model. For instance, the Viterbi algorithm of the first-order hidden Markov model can be easily derived as a special case of our conclusion when m = n = 1.
This method we analyzed is practical and valuable in its own right. Future research could use this method for applications in handwriting, speech recognition, speaker recognition, emotion recognition, and so forth.

## Conflict of Interests

The authors declare that there is no conflict of interests regarding the publication of this paper.

## Acknowledgments

This work is supported by the Major Program of the National Natural Science Foundation of China (no. 71390521), the Postdoctoral Science Foundation of China (no. 2014M551565), and the Scientific Research Foundation of Tongling University (no. 2012tlxyrc04).

## References

1.  J. Hu, M. K. Brown, and W. Turin, "HMM based on-line handwriting recognition," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 18, no. 10, pp. 1039–1045, 1996.
2.  M. S. Khorsheed, "Recognising handwritten Arabic manuscripts using a single hidden Markov model," _Pattern Recognition Letters_, vol. 24, no. 14, pp. 2235-2242, 2003.
3.  T. Artières, S. Marukatat, and P. Gallinari, "Online handwritten shape recognition using segmental hidden markov models," _IEEE Transactions on Pattern Analysis and Machine Intelligence_, vol. 29, no. 2, pp. 205-217, 2007.
4.  B. H. Juang and L. R. Rabiner, "Hidden Markov models for speech recognition," _Technometrics_, vol. 33, no. 3, pp. 251-272, 1991.
5.  M. Gales and S. Young, "The application of hidden Markov Models in speech recognition," _Foundations and Trends in Signal Processing_, vol. 1, no. 3, pp. 195-304, 2008.
6.  A. Löytynoja and M. C. Milinkovitch, “A hidden Markov model for progressive multiple alignment," _Bioinformatics_, vol. 19, no. 12, pp. 1505-1513, 2003.
7.  L. Regad, F. Guyon, J. Maupetit, P. Tufféry, and A. C. Camproux, "A Hidden Markov Model applied to the protein 3D structure analysis," _Computational Statistics and Data Analysis_, vol. 52, no. 6, pp. 3198-3207, 2008.
8.  R. M. Altman, "Mixed hidden Markov models: an extension of the hidden Markov model to the longitudinal data setting," _Journal of the American Statistical Association_, vol. 102, no. 477, pp. 201-210, 2007.
9.  A. Spagnoli, R. Henderson, R. J. Boys, and J. J. Houwing-Duistermaat, "A hidden Markov model for informative dropout in longitudinal response data with crisis states,” _Statistics and Probability Letters_, vol. 81, no. 7, pp. 730-738, 2011.
10. Y. Ephraim and N. Merhav, “Hidden Markov processes,” _IEEE Transactions on Information Theory_, vol. 48, no. 6, pp. 1518–1569, 2002.
11. J. A. Bilmes, "What HMMs can do," _IEICE Transactions on Information and Systems_, vol. E89-D, no. 3, pp. 869–891, 2006.
12. L. E. Baum and T. Petrie, "Statistical inference for probabilistic functions of finite state Markov chains," _The Annals of Mathematical Statistics_, vol. 37, no. 6, pp. 1554-1563, 1966.
13. L. R. Rabiner and B.-H. Juang, "An introduction to hidden Markov models," _IEEE ASSP Magazine_, vol. 3, no. 1, pp. 4-16, 1986.
14. J.-F. Mari, J.-P. Haton, and A. Kriouile, "Automatic word recognition based on second-order hidden Markov models," _IEEE Transactions on Speech and Audio Processing_, vol. 5, no. 1, pp. 22-25, 1997.
15. J.-F. Mari and F. Le Ber, "Temporal and spatial data mining with second-order hidden markov models," _Soft Computing_, vol. 10, no. 5, pp. 406-414, 2006.
16. L.-M. Lee, "High-order hidden markov model and application to continuous mandarin digit recognition,” _Journal of Information Science and Engineering_, vol. 27, no. 6, pp. 1919–1930, 2011.
17. J. A. du Preez, "Efficient training of high-order hidden Markov models using first-order representations” _Computer Speech and Language_, vol. 12, no. 1, pp. 23-39, 1998.
18. U. Hadar and H. Messer, "High-order hidden Markov models—estimation and implementation," in _Proceedings of the 15th IEEE/SP Workshop on Statistical Signal Processing (SSP '09)_, pp. 249-252, Cardiff, Wales, September 2009.
19. H. A. Engelbrecht and J. A. du Preez, "Efficient backward decoding of high-order hidden Markov models,” _Pattern Recognition_, vol. 43, no. 1, pp. 99–112, 2010.
20. F. Ye, N. Yi, and Y. F. Wang, “EM algorithm for training high-order hidden Markov model with multiple observation sequences," _Journal of Information and Computational Science_, vol. 8, no. 10, pp. 1761–1777, 2011.
